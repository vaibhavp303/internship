{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\n",
    "\n",
    "import time\n",
    "\n",
    "from selenium import webdriver\n",
    "\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import requests\n",
    "\n",
    "\\# Function to search for products on Amazon.in\n",
    "\n",
    "def search_amazon_product(product_name):\n",
    "\n",
    "\\# Set up the web driver (you need to have chromedriver installed and\n",
    "its path added to your system PATH)\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "\\# Open Amazon.in\n",
    "\n",
    "driver.get(\"https://www.amazon.in\")\n",
    "\n",
    "\\# Find the search input field and enter the product name\n",
    "\n",
    "search_box = driver.find_element_by_id(\"twotabsearchtextbox\")\n",
    "\n",
    "search_box.send_keys(product_name)\n",
    "\n",
    "search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "\\# Wait for the search results to load\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "\\# Get the HTML source of the search results page\n",
    "\n",
    "page_source = driver.page_source\n",
    "\n",
    "\\# Parse the HTML source using BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "\\# Find and print the product titles\n",
    "\n",
    "product_titles = soup.find_all('span', class\\_='a-text-normal')\n",
    "\n",
    "for title in product_titles:\n",
    "\n",
    "print(title.get_text())\n",
    "\n",
    "\\# Close the browser window\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "if \\_\\_name\\_\\_ == \"\\_\\_main\\_\\_\":\n",
    "\n",
    "\\# Get the product name from the user\n",
    "\n",
    "product_name = input(\"Enter the product to search for on Amazon.in: \")\n",
    "\n",
    "search_amazon_product(product_name)\n",
    "\n",
    "2\n",
    "\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\\# Function to scrape product details\n",
    "\n",
    "def scrape_product_details(url):\n",
    "\n",
    "try:\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "brand_name = soup.find('span', {'id': 'brand'}).text.strip()\n",
    "\n",
    "product_name = soup.find('h1', {'class': 'product-title'}).text.strip()\n",
    "\n",
    "price = soup.find('span', {'class': 'product-price'}).text.strip()\n",
    "\n",
    "return_exchange = soup.find('div', {'class':\n",
    "'return-exchange'}).text.strip()\n",
    "\n",
    "expected_delivery = soup.find('div', {'class':\n",
    "'expected-delivery'}).text.strip()\n",
    "\n",
    "availability = soup.find('div', {'class': 'availability'}).text.strip()\n",
    "\n",
    "return {\n",
    "\n",
    "'Brand Name': brand_name,\n",
    "\n",
    "'Name of the Product': product_name,\n",
    "\n",
    "'Price': price,\n",
    "\n",
    "'Return/Exchange': return_exchange,\n",
    "\n",
    "'Expected Delivery': expected_delivery,\n",
    "\n",
    "'Availability': availability,\n",
    "\n",
    "'Product URL': url\n",
    "\n",
    "}\n",
    "\n",
    "else:\n",
    "\n",
    "return None\n",
    "\n",
    "except Exception as e:\n",
    "\n",
    "print(f\"Error scraping product details: {e}\")\n",
    "\n",
    "return None\n",
    "\n",
    "\\# Function to scrape search results\n",
    "\n",
    "def scrape_search_results(search_query, num_pages=3):\n",
    "\n",
    "all_products = \\[\\]\n",
    "\n",
    "for page in range(1, num_pages + 1):\n",
    "\n",
    "search_url = f'https://example.com/search?q={search_query}&page={page}'\n",
    "\n",
    "response = requests.get(search_url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "product_links = soup.find_all('a', {'class': 'product-link'})\n",
    "\n",
    "for link in product_links:\n",
    "\n",
    "product_url = link\\['href'\\]\n",
    "\n",
    "product_data = scrape_product_details(product_url)\n",
    "\n",
    "if product_data:\n",
    "\n",
    "all_products.append(product_data)\n",
    "\n",
    "return all_products\n",
    "\n",
    "\\# Specify the search query and the number of pages to scrape\n",
    "\n",
    "search_query = 'your_search_query'\n",
    "\n",
    "num_pages = 3\n",
    "\n",
    "\\# Scrape the data\n",
    "\n",
    "products_data = scrape_search_results(search_query, num_pages)\n",
    "\n",
    "\\# Create a DataFrame\n",
    "\n",
    "df = pd.DataFrame(products_data)\n",
    "\n",
    "\\# Save the DataFrame to a CSV file\n",
    "\n",
    "df.to_csv('products_data.csv', index=False)\n",
    "\n",
    "3\n",
    "\n",
    "from selenium import webdriver\n",
    "\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import requests\n",
    "\n",
    "import os\n",
    "\n",
    "\\# Set the path for Chrome WebDriver\n",
    "\n",
    "chrome_driver_path = 'your_chromedriver_path_here'\n",
    "\n",
    "\\# Create a directory to save the images\n",
    "\n",
    "if not os.path.exists('downloaded_images'):\n",
    "\n",
    "os.makedirs('downloaded_images')\n",
    "\n",
    "\\# Function to download images\n",
    "\n",
    "def download_images(query, num_images):\n",
    "\n",
    "driver = webdriver.Chrome(executable_path=chrome_driver_path)\n",
    "\n",
    "driver.get(\"https://www.google.com/imghp\")\n",
    "\n",
    "\\# Locate the search bar and input the query\n",
    "\n",
    "search_box = driver.find_element_by_name(\"q\")\n",
    "\n",
    "search_box.clear()\n",
    "\n",
    "search_box.send_keys(query)\n",
    "\n",
    "search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "\\# Scroll down to load more images (you can increase this if needed)\n",
    "\n",
    "for \\_ in range(3):\n",
    "\n",
    "driver.execute_script(\"window.scrollBy(0, 1000)\")\n",
    "\n",
    "\\# Get the page source and parse it with BeautifulSoup\n",
    "\n",
    "page_source = driver.page_source\n",
    "\n",
    "soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "\n",
    "\\# Find all image elements on the page\n",
    "\n",
    "img_tags = soup.find_all(\"img\")\n",
    "\n",
    "\\# Download the first 'num_images' images\n",
    "\n",
    "for i, img_tag in enumerate(img_tags):\n",
    "\n",
    "if i \\>= num_images:\n",
    "\n",
    "break\n",
    "\n",
    "img_url = img_tag.get(\"src\")\n",
    "\n",
    "if img_url:\n",
    "\n",
    "response = requests.get(img_url)\n",
    "\n",
    "with open(f\"downloaded_images/{query}\\_{i+1}.jpg\", \"wb\") as img_file:\n",
    "\n",
    "img_file.write(response.content)\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "\\# Keywords and the number of images to download\n",
    "\n",
    "keywords = \\['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes'\\]\n",
    "\n",
    "num_images_per_keyword = 10\n",
    "\n",
    "for keyword in keywords:\n",
    "\n",
    "download_images(keyword, num_images_per_keyword)\n",
    "\n",
    "print(\"Downloaded images successfully!\")\n",
    "\n",
    "4\n",
    "\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\\# Function to extract details from a product listing\n",
    "\n",
    "def extract_product_details(product):\n",
    "\n",
    "details = {\n",
    "\n",
    "\"Brand Name\": \"-\",\n",
    "\n",
    "\"Smartphone Name\": \"-\",\n",
    "\n",
    "\"Colour\": \"-\",\n",
    "\n",
    "\"RAM\": \"-\",\n",
    "\n",
    "\"Storage(ROM)\": \"-\",\n",
    "\n",
    "\"Primary Camera\": \"-\",\n",
    "\n",
    "\"Secondary Camera\": \"-\",\n",
    "\n",
    "\"Display Size\": \"-\",\n",
    "\n",
    "\"Battery Capacity\": \"-\",\n",
    "\n",
    "\"Price\": \"-\",\n",
    "\n",
    "\"Product URL\": \"-\"\n",
    "\n",
    "}\n",
    "\n",
    "\\# Extracting product details if available\n",
    "\n",
    "title = product.find(\"div\", {\"class\": \"\\_4rR01T\"})\n",
    "\n",
    "if title:\n",
    "\n",
    "details\\[\"Smartphone Name\"\\] = title.text.strip()\n",
    "\n",
    "details\\[\"Brand Name\"\\] = details\\[\"Smartphone Name\"\\].split()\\[0\\]\n",
    "\n",
    "price = product.find(\"div\", {\"class\": \"\\_30jeq3 \\_1_WHN1\"})\n",
    "\n",
    "if price:\n",
    "\n",
    "details\\[\"Price\"\\] = price.text.strip()\n",
    "\n",
    "link = product.find(\"a\", {\"class\": \"IRpwTa\"})\n",
    "\n",
    "if link:\n",
    "\n",
    "details\\[\"Product URL\"\\] = \"https://www.flipkart.com\" + link\\[\"href\"\\]\n",
    "\n",
    "specs = product.find_all(\"li\", {\"class\": \"rgWa7D\"})\n",
    "\n",
    "for spec in specs:\n",
    "\n",
    "text = spec.text.strip()\n",
    "\n",
    "if \"RAM\" in text:\n",
    "\n",
    "details\\[\"RAM\"\\] = text\n",
    "\n",
    "elif \"Storage\" in text:\n",
    "\n",
    "details\\[\"Storage(ROM)\"\\] = text\n",
    "\n",
    "elif \"Primary Camera\" in text:\n",
    "\n",
    "details\\[\"Primary Camera\"\\] = text\n",
    "\n",
    "elif \"Secondary Camera\" in text:\n",
    "\n",
    "details\\[\"Secondary Camera\"\\] = text\n",
    "\n",
    "elif \"Display Size\" in text:\n",
    "\n",
    "details\\[\"Display Size\"\\] = text\n",
    "\n",
    "elif \"Battery Capacity\" in text:\n",
    "\n",
    "details\\[\"Battery Capacity\"\\] = text\n",
    "\n",
    "elif \"Color\" in text:\n",
    "\n",
    "details\\[\"Colour\"\\] = text\n",
    "\n",
    "return details\n",
    "\n",
    "\\# Main function to search and scrape Flipkart\n",
    "\n",
    "def scrape_flipkart(search_query):\n",
    "\n",
    "url = f\"https://www.flipkart.com/search?q={search_query.replace(' ',\n",
    "'+')}\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "products = soup.find_all(\"div\", {\"class\": \"\\_1AtVbE\"})\n",
    "\n",
    "data = \\[\\]\n",
    "\n",
    "for product in products:\n",
    "\n",
    "details = extract_product_details(product)\n",
    "\n",
    "data.append(details)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.to_csv(\"flipkart_smartphones.csv\", index=False)\n",
    "\n",
    "print(\"Data has been scraped and saved to flipkart_smartphones.csv\")\n",
    "\n",
    "else:\n",
    "\n",
    "print(\"Failed to retrieve the web page.\")\n",
    "\n",
    "if \\_\\_name\\_\\_ == \"\\_\\_main\\_\\_\":\n",
    "\n",
    "search_query = input(\"Enter the smartphone you want to search for on\n",
    "Flipkart: \")\n",
    "\n",
    "scrape_flipkart(search_query)\n",
    "\n",
    "5\n",
    "\n",
    "import requests\n",
    "\n",
    "def get_coordinates(city_name):\n",
    "\n",
    "\\# Replace 'YOUR_API_KEY' with your actual Google Maps API key\n",
    "\n",
    "api_key = 'YOUR_API_KEY'\n",
    "\n",
    "base_url = 'https://maps.googleapis.com/maps/api/geocode/json'\n",
    "\n",
    "params = {\n",
    "\n",
    "'address': city_name,\n",
    "\n",
    "'key': api_key\n",
    "\n",
    "}\n",
    "\n",
    "response = requests.get(base_url, params=params)\n",
    "\n",
    "if response.status_code == 200:\n",
    "\n",
    "data = response.json()\n",
    "\n",
    "if data\\['status'\\] == 'OK':\n",
    "\n",
    "location = data\\['results'\\]\\[0\\]\\['geometry'\\]\\['location'\\]\n",
    "\n",
    "latitude = location\\['lat'\\]\n",
    "\n",
    "longitude = location\\['lng'\\]\n",
    "\n",
    "return latitude, longitude\n",
    "\n",
    "else:\n",
    "\n",
    "print('Location not found')\n",
    "\n",
    "else:\n",
    "\n",
    "print('Failed to retrieve data')\n",
    "\n",
    "city_name = input('Enter a city name: ')\n",
    "\n",
    "coordinates = get_coordinates(city_name)\n",
    "\n",
    "if coordinates:\n",
    "\n",
    "print(f'Coordinates for {city_name}: Latitude={coordinates\\[0\\]},\n",
    "Longitude={coordinates\\[1\\]}')\n",
    "\n",
    "6import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\\# Define the URL of the page to scrape\n",
    "\n",
    "url = 'https://www.digit.in/top-products/best-gaming-laptops-40.html'\n",
    "\n",
    "\\# Send an HTTP GET request to the URL\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "\\# Check if the request was successful (status code 200)\n",
    "\n",
    "if response.status_code == 200:\n",
    "\n",
    "\\# Parse the HTML content of the page using BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "\\# Find the container that holds the laptop details\n",
    "\n",
    "laptop_container = soup.find('div', class\\_='TopNumbeHeading')\n",
    "\n",
    "\\# Extract information about each laptop\n",
    "\n",
    "laptop_details = \\[\\]\n",
    "\n",
    "for laptop in laptop_container.find_all('div', class\\_='Top-Rated'):\n",
    "\n",
    "laptop_name = laptop.find('div', class\\_='TopNumbeTitle').text.strip()\n",
    "\n",
    "laptop_specs = laptop.find('div', class\\_='TopNumbeList').text.strip()\n",
    "\n",
    "laptop_details.append({\n",
    "\n",
    "'Laptop Name': laptop_name,\n",
    "\n",
    "'Specifications': laptop_specs\n",
    "\n",
    "})\n",
    "\n",
    "\\# Print the details of each laptop\n",
    "\n",
    "for i, laptop in enumerate(laptop_details, start=1):\n",
    "\n",
    "print(f\"Laptop {i} Details:\")\n",
    "\n",
    "print(f\"Laptop Name: {laptop\\['Laptop Name'\\]}\")\n",
    "\n",
    "print(f\"Specifications: {laptop\\['Specifications'\\]}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "else:\n",
    "\n",
    "print(f\"Failed to retrieve the webpage. Status code:\n",
    "{response.status_code}\")\n",
    "\n",
    "7\n",
    "\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\\# Define the URL of the Forbes billionaires page\n",
    "\n",
    "url = \"https://www.forbes.com/billionaires/\"\n",
    "\n",
    "\\# Send an HTTP GET request to the URL\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "\\# Check if the request was successful\n",
    "\n",
    "if response.status_code == 200:\n",
    "\n",
    "\\# Parse the HTML content of the page using BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "\\# Find the table containing the billionaire details\n",
    "\n",
    "table = soup.find(\"table\", class\\_=\"table\")\n",
    "\n",
    "\\# Initialize empty lists to store the details\n",
    "\n",
    "ranks = \\[\\]\n",
    "\n",
    "names = \\[\\]\n",
    "\n",
    "net_worths = \\[\\]\n",
    "\n",
    "ages = \\[\\]\n",
    "\n",
    "citizenships = \\[\\]\n",
    "\n",
    "sources = \\[\\]\n",
    "\n",
    "industries = \\[\\]\n",
    "\n",
    "\\# Loop through each row in the table (skip the header row)\n",
    "\n",
    "for row in table.find_all(\"tr\")\\[1:\\]:\n",
    "\n",
    "\\# Extract data from each column\n",
    "\n",
    "columns = row.find_all(\"td\")\n",
    "\n",
    "rank = columns\\[0\\].text.strip()\n",
    "\n",
    "name = columns\\[1\\].text.strip()\n",
    "\n",
    "net_worth = columns\\[2\\].text.strip()\n",
    "\n",
    "age = columns\\[3\\].text.strip()\n",
    "\n",
    "citizenship = columns\\[4\\].text.strip()\n",
    "\n",
    "source = columns\\[5\\].text.strip()\n",
    "\n",
    "industry = columns\\[6\\].text.strip()\n",
    "\n",
    "\\# Append the data to the respective lists\n",
    "\n",
    "ranks.append(rank)\n",
    "\n",
    "names.append(name)\n",
    "\n",
    "net_worths.append(net_worth)\n",
    "\n",
    "ages.append(age)\n",
    "\n",
    "citizenships.append(citizenship)\n",
    "\n",
    "sources.append(source)\n",
    "\n",
    "industries.append(industry)\n",
    "\n",
    "\\# Print the first 10 billionaire details as an example\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "print(f\"Rank: {ranks\\[i\\]}\")\n",
    "\n",
    "print(f\"Name: {names\\[i\\]}\")\n",
    "\n",
    "print(f\"Net Worth: {net_worths\\[i\\]}\")\n",
    "\n",
    "print(f\"Age: {ages\\[i\\]}\")\n",
    "\n",
    "print(f\"Citizenship: {citizenships\\[i\\]}\")\n",
    "\n",
    "print(f\"Source: {sources\\[i\\]}\")\n",
    "\n",
    "print(f\"Industry: {industries\\[i\\]}\")\n",
    "\n",
    "print(\"-\" \\* 50)\n",
    "\n",
    "else:\n",
    "\n",
    "print(\"Failed to retrieve the page. Status code:\", response.status_code)\n",
    "\n",
    "8\n",
    "\n",
    "import time\n",
    "\n",
    "from selenium import webdriver\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\\# Set the URL of the YouTube video you want to scrape comments from\n",
    "\n",
    "video_url = \"https://www.youtube.com/watch?v=YOUR_VIDEO_ID\"\n",
    "\n",
    "\\# Initialize the Chrome WebDriver\n",
    "\n",
    "driver = webdriver.Chrome(executable_path='PATH_TO_CHROME_DRIVER')\n",
    "\n",
    "\\# Open the YouTube video page\n",
    "\n",
    "driver.get(video_url)\n",
    "\n",
    "\\# Scroll down to load more comments\n",
    "\n",
    "scroll_pause_time = 2 \\# Adjust this value as needed\n",
    "\n",
    "last_height = driver.execute_script(\"return\n",
    "document.documentElement.scrollHeight\")\n",
    "\n",
    "while True:\n",
    "\n",
    "driver.execute_script(\"window.scrollTo(0,\n",
    "document.documentElement.scrollHeight);\")\n",
    "\n",
    "time.sleep(scroll_pause_time)\n",
    "\n",
    "new_height = driver.execute_script(\"return\n",
    "document.documentElement.scrollHeight\")\n",
    "\n",
    "if new_height == last_height:\n",
    "\n",
    "break\n",
    "\n",
    "last_height = new_height\n",
    "\n",
    "\\# Extract comments, comment upvotes, and time of posting\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "comments = soup.find_all(\"yt-formatted-string\", {\"id\": \"content-text\"})\n",
    "\n",
    "upvotes = soup.find_all(\"span\", {\"id\": \"vote-count-middle\"})\n",
    "\n",
    "time_stamps = soup.find_all(\"a\", {\"class\": \"yt-simple-endpoint\n",
    "style-scope yt-formatted-string\"})\n",
    "\n",
    "\\# Print the first 500 comments, comment upvotes, and time stamps\n",
    "\n",
    "for i in range(500):\n",
    "\n",
    "print(\"Comment:\", comments\\[i\\].text)\n",
    "\n",
    "print(\"Upvotes:\", upvotes\\[i\\].text)\n",
    "\n",
    "print(\"Time:\", time_stamps\\[i\\].text)\n",
    "\n",
    "print(\"=========================================\")\n",
    "\n",
    "\\# Close the WebDriver\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "9\n",
    "\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\\# Function to scrape hostel data from a given URL\n",
    "\n",
    "def scrape_hostel_data(url):\n",
    "\n",
    "\\# Send an HTTP GET request to the URL\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "\\# Check if the request was successful\n",
    "\n",
    "if response.status_code != 200:\n",
    "\n",
    "print(\"Failed to fetch the webpage.\")\n",
    "\n",
    "return None\n",
    "\n",
    "\\# Parse the HTML content of the page\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "\\# Extract hostel information\n",
    "\n",
    "hostel_name = soup.find('h1', class\\_='title-2').text.strip()\n",
    "\n",
    "distance = soup.find('span', class\\_='description').text.strip()\n",
    "\n",
    "ratings = soup.find('div', class\\_='score orange').text.strip()\n",
    "\n",
    "total_reviews = soup.find('div', class\\_='reviews').text.strip()\n",
    "\n",
    "overall_reviews = soup.find('div', class\\_='keyword').text.strip()\n",
    "\n",
    "privates_price = soup.find('div', class\\_='price').text.strip()\n",
    "\n",
    "dorms_price = soup.find('div', class\\_='price dorm').text.strip()\n",
    "\n",
    "facilities = \\[item.text.strip() for item in soup.find_all('span',\n",
    "class\\_='ficon')\\]\n",
    "\n",
    "description = soup.find('div', class\\_='content\n",
    "collapse-content').text.strip()\n",
    "\n",
    "\\# Print the scraped data\n",
    "\n",
    "print(\"Hostel Name:\", hostel_name)\n",
    "\n",
    "print(\"Distance from City Centre:\", distance)\n",
    "\n",
    "print(\"Ratings:\", ratings)\n",
    "\n",
    "print(\"Total Reviews:\", total_reviews)\n",
    "\n",
    "print(\"Overall Reviews:\", overall_reviews)\n",
    "\n",
    "print(\"Privates from Price:\", privates_price)\n",
    "\n",
    "print(\"Dorms from Price:\", dorms_price)\n",
    "\n",
    "print(\"Facilities:\", ', '.join(facilities))\n",
    "\n",
    "print(\"Property Description:\", description)\n",
    "\n",
    "\\# URL of the London hostels page on Hostelworld\n",
    "\n",
    "url =\n",
    "\"https://www.hostelworld.com/s?search_keywords=London%2C+England&country=England&city=London&date_from=2023-10-01&date_to=2023-10-05\"\n",
    "\n",
    "\\# Call the function to scrape data from the given URL\n",
    "\n",
    "scrape_hostel_data(url)"
   ],
   "id": "42a14256-91c8-446e-92a7-ab6bf11055d3"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
