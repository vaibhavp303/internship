{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\\# Define the URL of Wikipedia\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/Main_Page\"\n",
    "\n",
    "\\# Send an HTTP GET request to the URL\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "\\# Check if the request was successful\n",
    "\n",
    "if response.status_code == 200:\n",
    "\n",
    "\\# Parse the HTML content of the page using BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "\\# Find all header tags (h1, h2, h3, h4, h5, h6)\n",
    "\n",
    "header_tags = soup.find_all(\\['h1', 'h2', 'h3', 'h4', 'h5', 'h6'\\])\n",
    "\n",
    "\\# Extract the text from the header tags\n",
    "\n",
    "header_text = \\[tag.text.strip() for tag in header_tags\\]\n",
    "\n",
    "\\# Create a DataFrame to store the header tags\n",
    "\n",
    "df = pd.DataFrame({'Header Tags': header_text})\n",
    "\n",
    "\\# Display the DataFrame\n",
    "\n",
    "print(df)\n",
    "\n",
    "else:\n",
    "\n",
    "print(\"Failed to retrieve the webpage. Status code:\",\n",
    "response.status_code)\n",
    "\n",
    "2 import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\\# Define the URL of the page\n",
    "\n",
    "url = \"https://presidentofindia.nic.in/former-presidents.htm\"\n",
    "\n",
    "\\# Send an HTTP GET request to the URL\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "\\# Check if the request was successful\n",
    "\n",
    "if response.status_code == 200:\n",
    "\n",
    "\\# Parse the HTML content of the page using BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "\\# Find the table containing the list of former presidents\n",
    "\n",
    "table = soup.find('table', {'class': 'tablepresident'})\n",
    "\n",
    "\\# Initialize lists to store the data\n",
    "\n",
    "names = \\[\\]\n",
    "\n",
    "terms = \\[\\]\n",
    "\n",
    "\\# Extract data from the table\n",
    "\n",
    "for row in table.find_all('tr')\\[1:\\]:\n",
    "\n",
    "columns = row.find_all('td')\n",
    "\n",
    "if len(columns) \\>= 2:\n",
    "\n",
    "name = columns\\[0\\].text.strip()\n",
    "\n",
    "term = columns\\[1\\].text.strip()\n",
    "\n",
    "names.append(name)\n",
    "\n",
    "terms.append(term)\n",
    "\n",
    "\\# Create a DataFrame from the extracted data\n",
    "\n",
    "data = {'Name': names, 'Term of Office': terms}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\\# Print the DataFrame\n",
    "\n",
    "print(df)\n",
    "\n",
    "else:\n",
    "\n",
    "print(\"Failed to retrieve data from the website.\")\n",
    "\n",
    "3 import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\\# Function to scrape and parse the data\n",
    "\n",
    "def scrape_icc_rankings(url):\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "return soup\n",
    "\n",
    "\\# Function to get the top 10 ODI teams\n",
    "\n",
    "def get_top_10_teams(soup):\n",
    "\n",
    "teams_data = soup.find_all('tr', class\\_='rankings-block\\_\\_banner') +\n",
    "soup.find_all('tr', class\\_='table-body')\n",
    "\n",
    "teams = \\[\\]\n",
    "\n",
    "for team in teams_data\\[:10\\]:\n",
    "\n",
    "data = team.find_all('td')\n",
    "\n",
    "team_name = data\\[1\\].text.strip()\n",
    "\n",
    "matches = data\\[2\\].text.strip()\n",
    "\n",
    "points = data\\[3\\].text.strip()\n",
    "\n",
    "rating = data\\[4\\].text.strip()\n",
    "\n",
    "teams.append({'Team': team_name, 'Matches': matches, 'Points': points,\n",
    "'Rating': rating})\n",
    "\n",
    "return teams\n",
    "\n",
    "\\# Function to get the top 10 ODI batsmen\n",
    "\n",
    "def get_top_10_batsmen(soup):\n",
    "\n",
    "batsmen_data = soup.find_all('div',\n",
    "class\\_='rankings-block\\_\\_top-player')\n",
    "\n",
    "batsmen = \\[\\]\n",
    "\n",
    "for player in batsmen_data\\[:10\\]:\n",
    "\n",
    "player_name = player.find('div',\n",
    "class\\_='rankings-block\\_\\_banner--name').text.strip()\n",
    "\n",
    "player_team = player.find('div',\n",
    "class\\_='rankings-block\\_\\_banner--nationality').text.strip()\n",
    "\n",
    "player_rating = player.find('div',\n",
    "class\\_='rankings-block\\_\\_banner--rating').text.strip()\n",
    "\n",
    "batsmen.append({'Batsman': player_name, 'Team': player_team, 'Rating':\n",
    "player_rating})\n",
    "\n",
    "return batsmen\n",
    "\n",
    "\\# Function to get the top 10 ODI bowlers\n",
    "\n",
    "def get_top_10_bowlers(soup):\n",
    "\n",
    "bowlers_data = soup.find_all('div',\n",
    "class\\_='rankings-block\\_\\_top-player')\n",
    "\n",
    "bowlers = \\[\\]\n",
    "\n",
    "for player in bowlers_data\\[10:20\\]:\n",
    "\n",
    "player_name = player.find('div',\n",
    "class\\_='rankings-block\\_\\_banner--name').text.strip()\n",
    "\n",
    "player_team = player.find('div',\n",
    "class\\_='rankings-block\\_\\_banner--nationality').text.strip()\n",
    "\n",
    "player_rating = player.find('div',\n",
    "class\\_='rankings-block\\_\\_banner--rating').text.strip()\n",
    "\n",
    "bowlers.append({'Bowler': player_name, 'Team': player_team, 'Rating':\n",
    "player_rating})\n",
    "\n",
    "return bowlers\n",
    "\n",
    "\\# Main function to fetch and display the data\n",
    "\n",
    "def main():\n",
    "\n",
    "url = 'https://www.icc-cricket.com/rankings/mens/team-rankings/odi'\n",
    "\n",
    "team_data = get_top_10_teams(scrape_icc_rankings(url))\n",
    "\n",
    "team_df = pd.DataFrame(team_data)\n",
    "\n",
    "url =\n",
    "'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting'\n",
    "\n",
    "batsmen_data = get_top_10_batsmen(scrape_icc_rankings(url))\n",
    "\n",
    "batsmen_df = pd.DataFrame(batsmen_data)\n",
    "\n",
    "url =\n",
    "'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling'\n",
    "\n",
    "bowlers_data = get_top_10_bowlers(scrape_icc_rankings(url))\n",
    "\n",
    "bowlers_df = pd.DataFrame(bowlers_data)\n",
    "\n",
    "print(\"Top 10 ODI Teams:\")\n",
    "\n",
    "print(team_df)\n",
    "\n",
    "print(\"\\nTop 10 ODI Batsmen:\")\n",
    "\n",
    "print(batsmen_df)\n",
    "\n",
    "print(\"\\nTop 10 ODI Bowlers:\")\n",
    "\n",
    "print(bowlers_df)\n",
    "\n",
    "if \\_\\_name\\_\\_ == \"\\_\\_main\\_\\_\":\n",
    "\n",
    "main()\n",
    "\n",
    "4 import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\\# Function to scrape the data\n",
    "\n",
    "def scrape_icc_cricket_rankings(url):\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "return soup\n",
    "\n",
    "\\# Function to scrape top 10 ODI teams\n",
    "\n",
    "def scrape_top_10_teams(soup):\n",
    "\n",
    "teams = \\[\\]\n",
    "\n",
    "matches = \\[\\]\n",
    "\n",
    "points = \\[\\]\n",
    "\n",
    "ratings = \\[\\]\n",
    "\n",
    "table = soup.find('table', class\\_='table')\n",
    "\n",
    "rows = table.find('tbody').find_all('tr')\n",
    "\n",
    "for row in rows\\[:10\\]:\n",
    "\n",
    "cols = row.find_all('td')\n",
    "\n",
    "team = cols\\[1\\].text.strip()\n",
    "\n",
    "match = cols\\[2\\].text.strip()\n",
    "\n",
    "point = cols\\[3\\].text.strip()\n",
    "\n",
    "rating = cols\\[4\\].text.strip()\n",
    "\n",
    "teams.append(team)\n",
    "\n",
    "matches.append(match)\n",
    "\n",
    "points.append(point)\n",
    "\n",
    "ratings.append(rating)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "\n",
    "'Team': teams,\n",
    "\n",
    "'Matches': matches,\n",
    "\n",
    "'Points': points,\n",
    "\n",
    "'Rating': ratings\n",
    "\n",
    "})\n",
    "\n",
    "return df\n",
    "\n",
    "\\# Function to scrape top 10 women's ODI batting players\n",
    "\n",
    "def scrape_top_10_batting_players(soup):\n",
    "\n",
    "players = \\[\\]\n",
    "\n",
    "teams = \\[\\]\n",
    "\n",
    "ratings = \\[\\]\n",
    "\n",
    "table = soup.find_all('table', class\\_='table')\\[1\\]\n",
    "\n",
    "rows = table.find('tbody').find_all('tr')\n",
    "\n",
    "for row in rows\\[:10\\]:\n",
    "\n",
    "cols = row.find_all('td')\n",
    "\n",
    "player = cols\\[1\\].text.strip()\n",
    "\n",
    "team = cols\\[2\\].text.strip()\n",
    "\n",
    "rating = cols\\[3\\].text.strip()\n",
    "\n",
    "players.append(player)\n",
    "\n",
    "teams.append(team)\n",
    "\n",
    "ratings.append(rating)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "\n",
    "'Player': players,\n",
    "\n",
    "'Team': teams,\n",
    "\n",
    "'Rating': ratings\n",
    "\n",
    "})\n",
    "\n",
    "return df\n",
    "\n",
    "\\# Function to scrape top 10 women's ODI all-rounders\n",
    "\n",
    "def scrape_top_10_allrounders(soup):\n",
    "\n",
    "players = \\[\\]\n",
    "\n",
    "teams = \\[\\]\n",
    "\n",
    "ratings = \\[\\]\n",
    "\n",
    "table = soup.find_all('table', class\\_='table')\\[2\\]\n",
    "\n",
    "rows = table.find('tbody').find_all('tr')\n",
    "\n",
    "for row in rows\\[:10\\]:\n",
    "\n",
    "cols = row.find_all('td')\n",
    "\n",
    "player = cols\\[1\\].text.strip()\n",
    "\n",
    "team = cols\\[2\\].text.strip()\n",
    "\n",
    "rating = cols\\[3\\].text.strip()\n",
    "\n",
    "players.append(player)\n",
    "\n",
    "teams.append(team)\n",
    "\n",
    "ratings.append(rating)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "\n",
    "'Player': players,\n",
    "\n",
    "'Team': teams,\n",
    "\n",
    "'Rating': ratings\n",
    "\n",
    "})\n",
    "\n",
    "return df\n",
    "\n",
    "if \\_\\_name\\_\\_ == \"\\_\\_main\\_\\_\":\n",
    "\n",
    "url = \"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\"\n",
    "\n",
    "team_soup = scrape_icc_cricket_rankings(url)\n",
    "\n",
    "top_10_teams_df = scrape_top_10_teams(team_soup)\n",
    "\n",
    "url =\n",
    "\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\"\n",
    "\n",
    "batting_soup = scrape_icc_cricket_rankings(url)\n",
    "\n",
    "top_10_batting_players_df = scrape_top_10_batting_players(batting_soup)\n",
    "\n",
    "url =\n",
    "\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\"\n",
    "\n",
    "allrounder_soup = scrape_icc_cricket_rankings(url)\n",
    "\n",
    "top_10_allrounders_df = scrape_top_10_allrounders(allrounder_soup)\n",
    "\n",
    "print(\"Top 10 ODI Women's Cricket Teams:\")\n",
    "\n",
    "print(top_10_teams_df)\n",
    "\n",
    "print(\"\\nTop 10 Women's ODI Batting Players:\")\n",
    "\n",
    "print(top_10_batting_players_df)\n",
    "\n",
    "print(\"\\nTop 10 Women's ODI All-rounders:\")\n",
    "\n",
    "print(top_10_allrounders_df)\n",
    "\n",
    "5 import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\\# URL of the CNBC world news page\n",
    "\n",
    "url = \"https://www.cnbc.com/world/?region=world\"\n",
    "\n",
    "\\# Send an HTTP GET request to the URL\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "\\# Check if the request was successful\n",
    "\n",
    "if response.status_code == 200:\n",
    "\n",
    "\\# Parse the HTML content of the page using BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "\\# Find all the news articles on the page\n",
    "\n",
    "articles = soup.find_all(\"div\", class\\_=\"Card-title\")\n",
    "\n",
    "\\# Initialize lists to store data\n",
    "\n",
    "headlines = \\[\\]\n",
    "\n",
    "times = \\[\\]\n",
    "\n",
    "news_links = \\[\\]\n",
    "\n",
    "\\# Extract data from each article\n",
    "\n",
    "for article in articles:\n",
    "\n",
    "\\# Extract headline\n",
    "\n",
    "headline = article.find(\"a\", class\\_=\"Card-title-link\")\n",
    "\n",
    "if headline:\n",
    "\n",
    "headlines.append(headline.text.strip())\n",
    "\n",
    "else:\n",
    "\n",
    "headlines.append(\"N/A\")\n",
    "\n",
    "\\# Extract time\n",
    "\n",
    "time = article.find(\"time\", class\\_=\"Card-time\")\n",
    "\n",
    "if time:\n",
    "\n",
    "times.append(time\\[\"datetime\"\\])\n",
    "\n",
    "else:\n",
    "\n",
    "times.append(\"N/A\")\n",
    "\n",
    "\\# Extract news link\n",
    "\n",
    "link = article.find(\"a\", class\\_=\"Card-title-link\")\n",
    "\n",
    "if link:\n",
    "\n",
    "news_links.append(link\\[\"href\"\\])\n",
    "\n",
    "else:\n",
    "\n",
    "news_links.append(\"N/A\")\n",
    "\n",
    "\\# Create a DataFrame\n",
    "\n",
    "df = pd.DataFrame({\n",
    "\n",
    "\"Headline\": headlines,\n",
    "\n",
    "\"Time\": times,\n",
    "\n",
    "\"News Link\": news_links\n",
    "\n",
    "})\n",
    "\n",
    "\\# Display the DataFrame\n",
    "\n",
    "print(df)\n",
    "\n",
    "else:\n",
    "\n",
    "print(\"Failed to retrieve the web page. Status code:\",\n",
    "response.status_code)\n",
    "\n",
    "6 import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\\# Define the URL of the page to scrape\n",
    "\n",
    "url =\n",
    "\"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "\n",
    "\\# Send an HTTP GET request to the URL\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "\\# Check if the request was successful\n",
    "\n",
    "if response.status_code == 200:\n",
    "\n",
    "\\# Parse the HTML content of the page\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "\\# Find the section containing the list of articles\n",
    "\n",
    "articles_section = soup.find(\"section\", {\"id\":\n",
    "\"most-downloaded-articles\"})\n",
    "\n",
    "\\# Initialize lists to store data\n",
    "\n",
    "titles = \\[\\]\n",
    "\n",
    "authors = \\[\\]\n",
    "\n",
    "dates = \\[\\]\n",
    "\n",
    "paper_urls = \\[\\]\n",
    "\n",
    "\\# Loop through each article in the list\n",
    "\n",
    "for article in articles_section.find_all(\"li\"):\n",
    "\n",
    "\\# Extract title, authors, date, and paper URL\n",
    "\n",
    "title = article.find(\"h2\").text.strip()\n",
    "\n",
    "author = article.find(\"div\", {\"class\": \"authors\"}).text.strip()\n",
    "\n",
    "date = article.find(\"div\", {\"class\":\n",
    "\"article-item\\_\\_date\"}).text.strip()\n",
    "\n",
    "paper_url = article.find(\"a\", {\"class\":\n",
    "\"article-content-title\"})\\[\"href\"\\]\n",
    "\n",
    "\\# Append data to respective lists\n",
    "\n",
    "titles.append(title)\n",
    "\n",
    "authors.append(author)\n",
    "\n",
    "dates.append(date)\n",
    "\n",
    "paper_urls.append(paper_url)\n",
    "\n",
    "\\# Create a DataFrame from the scraped data\n",
    "\n",
    "df = pd.DataFrame({\n",
    "\n",
    "\"Paper Title\": titles,\n",
    "\n",
    "\"Authors\": authors,\n",
    "\n",
    "\"Published Date\": dates,\n",
    "\n",
    "\"Paper URL\": paper_urls\n",
    "\n",
    "})\n",
    "\n",
    "\\# Print the DataFrame\n",
    "\n",
    "print(df)\n",
    "\n",
    "else:\n",
    "\n",
    "print(\"Failed to retrieve the webpage.\")\n",
    "\n",
    "7 import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\\# Function to scrape the data\n",
    "\n",
    "def scrape_dineout_data():\n",
    "\n",
    "url = \"https://www.dineout.co.in/delhi-restaurants\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code != 200:\n",
    "\n",
    "print(\"Failed to fetch the webpage\")\n",
    "\n",
    "return None\n",
    "\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "restaurant_data = \\[\\]\n",
    "\n",
    "\\# Find all the restaurant containers\n",
    "\n",
    "restaurant_containers = soup.find_all(\"div\", class\\_=\"restnt-info\")\n",
    "\n",
    "for container in restaurant_containers:\n",
    "\n",
    "name = container.find(\"div\", class\\_=\"restnt-info cursor\")\n",
    "\n",
    "cuisine = container.find(\"span\", class\\_=\"double-line-ellipsis\")\n",
    "\n",
    "location = container.find(\"div\", class\\_=\"restnt-loc ellipsis\")\n",
    "\n",
    "ratings = container.find(\"span\", class\\_=\"star-text\")\n",
    "\n",
    "img_url = container.find(\"img\")\\[\"src\"\\]\n",
    "\n",
    "if name and cuisine and location and ratings:\n",
    "\n",
    "restaurant_data.append({\n",
    "\n",
    "\"Restaurant Name\": name.text.strip(),\n",
    "\n",
    "\"Cuisine\": cuisine.text.strip(),\n",
    "\n",
    "\"Location\": location.text.strip(),\n",
    "\n",
    "\"Ratings\": ratings.text.strip(),\n",
    "\n",
    "\"Image URL\": img_url\n",
    "\n",
    "})\n",
    "\n",
    "return restaurant_data\n",
    "\n",
    "\\# Create a DataFrame from the scraped data\n",
    "\n",
    "def create_dataframe(data):\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "return df\n",
    "\n",
    "\\# Main function\n",
    "\n",
    "if \\_\\_name\\_\\_ == \"\\_\\_main\\_\\_\":\n",
    "\n",
    "restaurant_data = scrape_dineout_data()\n",
    "\n",
    "if restaurant_data:\n",
    "\n",
    "df = create_dataframe(restaurant_data)\n",
    "\n",
    "print(df)ss"
   ],
   "id": "42a14256-91c8-446e-92a7-ab6bf11055d3"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
